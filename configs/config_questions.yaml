input_paths:
  prompt_dir: data_generation/prompts
  context_file: /home/dreich/Projects/context.jsonl
llm_gen_config:
  model_setup:
    use_cloud: False
    model_name: meta-llama/Llama-3.2-1B-Instruct
    prompts:
      user_prompt: user_prompts/gen_questions_en.jinja2
      user_prompt_de: user_prompts/gen_questions_de.jinja2
      system_prompt: system_prompts/gen_qa.jinja2
      chat_template: chat_templates/Llama-3.1-8B-Instruct.jinja2
  inference_config:
    batch_size: 16
    batch_size_dl: 128
    llm_sampling_params:
      max_tokens: 512
      temperature: 0.7
      top_p: 0.8
      skip_special_tokens: True
    vllm_engine_params:
      max_seq_len_to_capture: 6144
      max_model_len: 6144
      tensor_parallel_size: 1
